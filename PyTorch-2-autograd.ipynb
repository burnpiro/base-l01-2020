{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch: Autograd\n",
    "====\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autograd** is PyTorch' package for automatic differentiation for all operations on Tensors. It's a *define-by-run* framework - backpropagation is defined by how the code runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.requires_grad = True`\n",
    "----\n",
    "\n",
    "This attribute sets the tensor to track all operations on it. After finishing computation you can then call `.backward()` to automatically compute all the gradients and store them into `.grad` attribute of each tensor.\n",
    "\n",
    "`.detach()` stops the tensor from tracking history, preventing future computation from being tracked.\n",
    "\n",
    "To stop tracking history in a block of code you can wrap it in `with torch.no_grad():`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Function`\n",
    "----\n",
    "\n",
    "Every operation performed on a Tensor creates a new `Function` object, that performs the computation and records that it happened. Alltogether they build up an acyclic graph, encoding a complete history of computation. Tensor's attribute `.grad_fn` refers to a `Function` used to create the Tensor (except for Tensors created by the user, where `.grad_fn is None`).\n",
    "\n",
    "If you want to compute the derivatives, you can call `.backward()` on a `Tensor`. If the `Tensor` is a scalar (holds one-element data) there is no need to pass any arguments to `.backward()`. If you are using a vector, you need to specify a `gradient` argument, which is a tensor of a matching shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor with requires_grad\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a simple operation and check the `grad_fn`\n",
    "y = x - 4\n",
    "print(y.grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform some more operations\n",
    "z = y * y * 5\n",
    "out = z.mean()\n",
    "\n",
    "print(z)  # see the grad_fn\n",
    "print(out)  # see the grad_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as earlier, `.requires_grad_(...) changes the flag in-place\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)  # <- this will be None\n",
    "print('----')\n",
    "\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform backpropagation on `out` and calculate the gradients\n",
    "# since `out` contains a single scalar, there is no need to pass arguments\n",
    "out.backward()\n",
    "# equivalent: out.backward(torch.tensor(1.))\n",
    "print(x.grad)  # d(out)/dx\n",
    "\n",
    "# .backward() accumulates gradient only in the leaf nodes\n",
    "# that is why for y, z the grad is None\n",
    "print(y.grad)\n",
    "print(z.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small mathematical note\n",
    "\n",
    "Let `out` be called $o$.\n",
    "\n",
    "$$o = \\frac{1}{4} \\sum_i{z_i}$$\n",
    "$$z_i = 5*\\left(x_i - 4\\right)^2$$\n",
    "$$z_i\\mid_{x_i=1} = 27$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\\frac{\\partial o}{\\partial x_i} = \\frac{5}{2} \\left(x_i - 4\\right)$$\n",
    "$$\\frac{\\partial o}{\\partial x_i}\\mid_{x_i=1} = -\\frac{15}{2} = -7.5$$\n",
    "\n",
    "## Gradient\n",
    "\n",
    "For a vector valued function $\\vec{y}=f\\left(\\vec{x}\\right)$, the gradient of $\\vec{y}$ with respect to $\\vec{x}$ is a Jacobian matrix:\n",
    "\n",
    "$$J = \\left(\\begin{array}{ccc}\\frac{\\partial y_1}{\\partial x_1} & \\dots & \\frac{\\partial y_1}{\\partial x_n}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\dots & \\frac{\\partial y_m}{\\partial x_n} \\end{array}\\right)$$\n",
    "\n",
    "`torch.autograd` is an engine for computing vector-Jacobian product - given any vector $v = \\left(\\begin{array}{cccc}v_1 & v_2 & \\dots & v_m\\end{array}\\right)^T$ compute a product $v^T \\cdot J$. If $v$ is a gradient of a scalar function $l=g\\left(\\vec{y}\\right)$ (that is $v = \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_1}& \\dots & \\frac{\\partial l}{\\partial y_m}\\end{array}\\right)^T$), then, by chain rule, the vector-Jacobian product would be the gradient of $l$ with respect to $\\vec{x}$:\n",
    "\n",
    "$$J^T \\cdot v = \\left(\\begin{array}{ccc}\\frac{\\partial y_1}{\\partial x_1} & \\dots & \\frac{\\partial y_m}{\\partial x_1}\\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_1}{\\partial x_n} & \\dots & \\frac{\\partial y_m}{\\partial x_n} \\end{array}\\right) \\left(\\begin{array}{c}\\frac{\\partial l}{\\partial y_1} \\\\ \\vdots \\\\ \\frac{\\partial l}{\\partial y_m}\\end{array}\\right)=\\left(\\begin{array}{c}\\frac{\\partial l}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial l}{\\partial x_n}\\end{array}\\right)$$\n",
    "\n",
    "> Note: $v^T \\cdot J$ gives a row vector which can be treated as a column vector by taking $J^T \\cdot v$.\n",
    "\n",
    "This characteristic of vector-Jacobian product makes it very convenient to feed external gradients into a model that has non-scalar output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of vector-Jacobian product\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000: # L2 norm\n",
    "    print(f'L2 norm: {y.data.norm()}')\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is not a scalar - cannot calculate the Jacobian directly\n",
    "# you need to pass a vector as an argument - 3 element\n",
    "v = torch.tensor([0.1, 1.0, 0.00001], dtype=torch.float)  # e.g. from a loss function\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop autograd from tracking history\n",
    "print(x.requires_grad)\n",
    "print((2 * x).requires_grad)\n",
    "print('-----')\n",
    "\n",
    "# torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    print((2 * x).requires_grad)\n",
    "print('-----')\n",
    "    \n",
    "# .detach()\n",
    "detached = x.detach()\n",
    "print(detached.requires_grad)\n",
    "print((detached == x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create 3 torch Tensors (scalars): $x = 1$, $w = 0.27$ and $b = 3$, so that they will be tracking gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Calculate the following equation:\n",
    "\n",
    "$$y = w \\cdot x + b$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute and display the gradients for each value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate the result of another equation, compute gradients and display them.\n",
    "\n",
    "$$z = w \\cdot \\left(x ^ 2 - b\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
