{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Linear regression\n",
    "==============\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Linear regression is a linear approach to modelling relationship between a **dependent variable** (scalar response) and one or more **independent variables** (explanatory variables).\n",
    "\n",
    "The relationships are modeled using *linear predictor functions* (linear combination of a set of coefficients and explanatory variables). Their parameters (coefficients) are estimated from the data. For example, with one explanatory variable $x$ and a scalar response $x$, the linear regression is expressed as:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 \\cdot x$$\n",
    "\n",
    "where *betas* are the coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More generally, the *model* is a function that performs a matrix multiplication of the input $X$ and the weights $\\mathbf{\\beta}$ (coefficients) and adds the bias $\\epsilon$.\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\varepsilon_i = \\mathbf{x_i}^T\\mathbf{\\beta} + \\varepsilon_i,\\: i=1,\\dots,n$$\n",
    "\n",
    "in matrix notation:\n",
    "\n",
    "$$\\mathbf{y} = X\\mathbf{\\beta} + \\mathbf{\\varepsilon}$$\n",
    "\n",
    "where: \n",
    "\n",
    "* $\\mathbf{y}$ is a vector of observed values of the measured variable (dependent variable),\n",
    "$$\\mathbf{y} = \\left(\\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array}\\right)$$\n",
    "\n",
    "* $\\mathbf{X}$ is a matrix of row-vectors $\\mathbf{x}_i$, that is observed values of random variables (independent variables) for each corresponding regressand value $y_i$\n",
    "$$\\mathbf{X} = \\left(\\begin{array}{c} \\mathbf{x}_1^T \\\\ \\mathbf{x}_2^T \\\\ \\vdots \\\\ \\mathbf{x}_n^T \\end{array}\\right)=\\left(\\begin{array}{cccc} 1 & x_{11} & \\dots & x_{1p} \\\\ 1 & x_{21} & \\dots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & \\dots & x_{np} \\end{array}\\right)$$\n",
    "\n",
    "* $\\mathbf{\\beta}$ is a $(p+1)$-dimensional *parameter vector*, i.e. a vector of coefficients, where $\\beta_0$ is usually called the *intercept term* - multiplied by $x_{i0} = 1$; intercept term is not obligatory - without it (i.e., with no $1$ in the row-vector $\\mathbf{x}_i$), the parameter vector has $p$ parameters,\n",
    "$$\\mathbf{\\beta} = \\left(\\begin{array}{c} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{array}\\right)$$\n",
    "\n",
    "* $\\mathbf{\\varepsilon}$ is a vector called the *error term* or *noise*, which captures all factors influencing the dependent variable $y$ other than the regressors $\\mathbf{x}$\n",
    "$$\\mathbf{\\varepsilon} = \\left(\\begin{array}{c} \\varepsilon_0 \\\\ \\varepsilon_1 \\\\ \\vdots \\\\ \\varepsilon_n \\end{array}\\right)$$\n",
    "\n",
    "Linear regression focuses on the conditional probability distribution of the response given the values of the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The loss is the error between the predicted value $\\hat{y}$ and the ground-truth $y$. The loss can be calculated using various functions, depending on the estimation methods. One of the most common estimation technique is least-squares estimation.\n",
    "\n",
    "This method utilizes Mean Squared Error (MSE), which is an average squared error (difference between predicted value and ground-truth), averaged for every example:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=0}^n \\left(y_i - \\hat{y}_i\\right)^2$$\n",
    "$$\\hat{y}_i=\\mathbf{\\beta}\\times\\mathbf{x_i}$$\n",
    "\n",
    "The loss is used by the Gradient Descent algorithm. It optimizes the values of coefficients to find the minimum value of the loss function.\n",
    "\n",
    "$$\\mathbf{\\hat{\\beta}} = \\arg_\\hat{\\beta}\\min MSE$$\n",
    "\n",
    "![image](https://miro.medium.com/max/2400/1*N5WjbzwsCFse-KPjBWZZ6g.jpeg)\n",
    "\n",
    "How it works? Using the value of the gradient (i.e. the steepness of the slope), the algorithm applies changes to the weights, to make steps of appropriate size towards the optimum.\n",
    "\n",
    "If a gradient element is **positive** the loss function can be decreased by **decreasing** the element's value. On the other hand, with **negative** gradient the loss will be decreased by **increasing** the element's value slightly. The increase and decrease of values is proportional to the steepness (value of the gradient) and the **learning rate** $\\alpha$.\n",
    "\n",
    "![image](https://i.imgur.com/2H4INoV.png)\n",
    "![image](https://i.imgur.com/h7E2uAv.png)\n",
    "\n",
    "The Gradient Descent algorithm includes the following steps:\n",
    "\n",
    "  1. Generate predictions\n",
    "  2. Calculate the loss\n",
    "  3. Compute gradients w.r.t the weights and biases\n",
    "  4. Adjust the weights by subtracting values proportional to the gradient\n",
    "  5. Reset the gradients to zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import trange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# create a toy dataset\n",
    "X, y = make_regression(n_samples=100,\n",
    "                       n_features=1,\n",
    "                       noise=20,\n",
    "                       random_state=13)\n",
    "y = y[:, np.newaxis]  # column vector\n",
    "\n",
    "def plot(X, y, model=None, loss=None):\n",
    "    with torch.no_grad():\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xlabel(\"independent variable\")\n",
    "        ax.set_ylabel(\"dependent variable\")\n",
    "        ax.scatter(X, y, label=\"data\")\n",
    "        if model is not None:\n",
    "            inputs = torch.from_numpy(X.astype(np.float32))\n",
    "            targets = torch.from_numpy(y.astype(np.float32))\n",
    "            preds = model(inputs)\n",
    "            ax.plot(X, preds.numpy(), label=\"predictions\")\n",
    "            if loss is not None:\n",
    "                print(f\"Loss: {loss(preds, targets)}\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "plot(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# convert data to torch.Tensor - numpy uses double as default, whereas for PyTorch float32 is default\n",
    "inputs = torch.from_numpy(X.astype(np.float32))\n",
    "targets = torch.from_numpy(y.astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear regression from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Create weights `w` == $\\mathbf{\\beta}$ and biases `b` == $\\mathbf{\\varepsilon}$ tensors of appropriate shape and random values for linear regression (according to the dimensions of input data). Do not use intercept term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "w = torch.randn(1, inputs.size()[-1], requires_grad=True)\n",
    "b = torch.randn(targets.size()[-1], requires_grad=True)\n",
    "print(w)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_weights_and_biases(weights, biases, data, target):\n",
    "    x_weights, y_weights = weights.size()\n",
    "    assert x_weights == target.size()[-1]\n",
    "    assert y_weights == data.size()[-1]\n",
    "    assert biases.size() == target.size()[-1:]\n",
    "\n",
    "test_weights_and_biases(w, b, inputs, targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. Prepare model function, which will perform operation:\n",
    "$$\\mathbf{y} = \\mathbf{X} \\times \\mathbf{\\beta}^T + \\mathbf{\\varepsilon}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def model(x, w=w, b=b):\n",
    "    return x @ w.t() + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_model_fn():\n",
    "    x = torch.tensor([[1, 2], [2, 3], [3, 4]])\n",
    "    w = torch.tensor([[5, 6]])\n",
    "    b = torch.tensor([[1], [2], [-1]])\n",
    "    assert (model(x, w=w, b=b) == torch.tensor([[18], [30], [38]])).all()\n",
    "    \n",
    "test_model_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Generate model predictions for inputs and plot using matplotlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot(X, y, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Define MSE loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def mse(pred, true):\n",
    "    diff = pred - true\n",
    "    return torch.sum(diff * diff) / diff.numel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def test_mse():\n",
    "    preds = torch.tensor([1, 2, 4])\n",
    "    trues = torch.tensor([3, 3, 3])\n",
    "    assert mse(preds, trues) == torch.tensor(2.)\n",
    "\n",
    "test_mse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. Calculate and display loss for model predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "loss = mse(model(inputs), targets)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Compute the gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4. Subtract gradients multiplied by learning rate $\\alpha=1\\cdot10^{-3}$ from weights and biases and zero the gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * lr\n",
    "    b -= b.grad * lr\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot prediction with loss value\n",
    "plot(X, y, model, mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5. Use the above steps to train the model for 100 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "pbar = trange(num_epochs, desc=\"Training\", leave=True)\n",
    "for epoch in pbar:\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * lr\n",
    "        b -= b.grad * lr\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    pbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    pbar.refresh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# calculate new loss and plot predictions\n",
    "plot(X, y, model, mse)\n",
    "    \n",
    "# further training or modifying learning rate will enhance the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear regression using PyTorch built-ins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# hyper-params\n",
    "input_size = inputs.size()[-1]\n",
    "output_size = targets.size()[-1]\n",
    "num_epochs = 100\n",
    "lr = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define linear regression model\n",
    "model = nn.Linear(input_size, output_size)\n",
    "print(model.weight)\n",
    "print(model.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# define criterion (loss function) and SGD optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pbar = trange(num_epochs, desc=\"Training\", leave=True)\n",
    "for epoch in pbar:\n",
    "    preds = model(inputs)\n",
    "    loss = criterion(preds, targets)\n",
    "    \n",
    "    # backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pbar.set_description(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    pbar.refresh()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# calculate new loss and plot predictions\n",
    "plot(X, y, model, mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Exercises\n",
    "-----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Modify the params of the `make_regression` function to have more than one independent value. Train the model on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. Use `sklearn.datasets.load_boston` to load boston house-prices dataset for regression and train linear regression model. Tune the hyperparams to achieve a good result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Prepare training function that will train the model and plot how the MSE changes through epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, inputs, outputs, num_epochs, lr):\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
